@book{ertelIntroductionArtificialIntelligence2017,
  title = {Introduction to {{Artificial Intelligence}}},
  author = {Ertel, Wolfgang},
  date = {2017},
  series = {Undergraduate {{Topics}} in {{Computer Science}}},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-58487-4},
  isbn = {978-3-319-58487-4},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\6NLNSJPS\\Ertel_2017_Introduction to Artificial Intelligence_annotated.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\ZGWAS38E\\Ertel_2017_Introduction to Artificial Intelligence.pdf}
}

@book{russellArtificialIntelligenceModern2021,
  title = {Artificial Intelligence: A Modern Approach},
  shorttitle = {Artificial Intelligence},
  author = {Russell, Stuart J. and Norvig, Peter},
  date = {2021},
  series = {Pearson Series in Artificial Intelligence},
  edition = {Fourth edition},
  publisher = {{Pearson}},
  location = {{Hoboken}},
  abstract = {"Updated edition of popular textbook on Artificial Intelligence. This edition specific looks at ways of keeping artificial intelligence under control"--},
  isbn = {978-0-13-461099-3},
  keywords = {Artificial intelligence},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\CE94W955\\Russell_Norvig_2021_Artificial intelligence.pdf}
}

@book{suttonReinforcementLearningIntroduction2018,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  date = {2018},
  series = {Adaptive Computation and Machine Learning Series},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  location = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  isbn = {978-0-262-03924-6},
  langid = {english},
  pagetotal = {526},
  keywords = {Reinforcement learning},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\PP56DKTH\\Sutton_Barto_2018_Reinforcement learning.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\Z35AZ227\\Sutton_Barto_2018_Reinforcement learning_annotated.pdf}
}

@inproceedings{vanderreeReinforcementLearningGame2013,
  title = {Reinforcement Learning in the Game of {{Othello}}: {{Learning}} against a Fixed Opponent and Learning from Self-Play},
  shorttitle = {Reinforcement Learning in the Game of {{Othello}}},
  booktitle = {2013 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  author = {van der Ree, Michiel and Wiering, Marco},
  options = {useprefix=true},
  date = {2013-04},
  pages = {108--115},
  publisher = {{IEEE}},
  location = {{Singapore}},
  doi = {10.1109/ADPRL.2013.6614996},
  eventtitle = {2013 {{IEEE Symposium}} on {{Adaptive Dynamic Programming}} and {{Reinforcement Learning}} ({{ADPRL}})},
  isbn = {978-1-4673-5925-2},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\FMDT2E79\\van der Ree_Wiering_2013_Reinforcement learning in the game of Othello.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\UBIECW8B\\van der Ree_Wiering_2013_Reinforcement learning in the game of Othello_annotated.pdf}
}

@incollection{tesauroTemporalDifferenceLearning1992a,
  title = {Temporal {{Difference Learning}} of {{Backgammon Strategy}}},
  booktitle = {Machine {{Learning Proceedings}} 1992},
  author = {Tesauro, Gerald},
  date = {1992},
  pages = {451--457},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-55860-247-2.50063-2},
  abstract = {This paper presents a case study in which the TD(A) algorithm for training connectionist networks, proposed in (Sutton, 1988), is applied to learning the game of backgammon from the outcome of self-play. This is apparently the first application of this algorithm to a complex nontrivial task. It is found that, with zero knowledge built in, networks are able to learn from scratch to play the entire game at a fairly strong intermediate level of performance, which is clearly better than conventional commercial programs, and which in fact surpasses comparable networks trained on a massive human expert data set. The hidden units in these network have apparently discovered useful features, a longstanding goal of computer games research. Furthermore, when a set of handcrafted features is added to the input representation, the resulting networks reach a near-expert level of performance, and have achieved good results in tests against worldclass human play.},
  isbn = {978-1-55860-247-2},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\VAJWTM3T\\Tesauro - 1992 - Temporal Difference Learning of Backgammon Strateg.pdf}
}

@incollection{szitaReinforcementLearningGames2012,
  title = {Reinforcement {{Learning}} in {{Games}}},
  booktitle = {Reinforcement {{Learning}}},
  author = {Szita, István},
  editor = {Wiering, Marco and van Otterlo, Martijn},
  options = {useprefix=true},
  date = {2012},
  series = {Adaptation, {{Learning}}, and {{Optimization}}},
  volume = {12},
  pages = {539--577},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-27645-3_17},
  abstract = {Reinforcement learning and games have a long and mutually beneficial common history. From one side, games are rich and challenging domains for testing reinforcement learning algorithms. From the other side, in several games the best computer players use reinforcement learning. The chapter begins with a selection of games and notable reinforcement learning implementations. Without any modifications, the basic reinforcement learning algorithms are rarely sufficient for high-level gameplay, so it is essential to discuss the additional ideas, ways of inserting domain knowledge, implementation decisions that are necessary for scaling up. These are reviewed in sufficient detail to understand their potentials and their limitations. The second part of the chapter lists challenges for reinforcement learning in games, together with a review of proposed solution methods. While this listing has a game-centric viewpoint, and some of the items are specific to games (like opponent modelling), a large portion of this overview can provide insight for other kinds of applications, too. In the third part we review how reinforcement learning can be useful in game development and find its way into commercial computer games. Finally, we provide pointers for more in-depth reviews of specific games and solution approaches.},
  isbn = {978-3-642-27645-3},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\LJNUCBQS\\Szita - 2012 - Reinforcement Learning in Games.pdf}
}

@inproceedings{samsudenReviewPaperImplementing2019,
  title = {A {{Review Paper}} on {{Implementing Reinforcement Learning Technique}} in {{Optimising Games Performance}}},
  booktitle = {2019 {{IEEE}} 9th {{International Conference}} on {{System Engineering}} and {{Technology}} ({{ICSET}})},
  author = {Samsuden, Mohd Azmin and Diah, Norizan Mat and Rahman, Nurazzah Abdul},
  date = {2019-10},
  pages = {258--263},
  publisher = {{IEEE}},
  location = {{Shah Alam, Malaysia}},
  doi = {10.1109/ICSEngT.2019.8906400},
  eventtitle = {2019 {{IEEE}} 9th {{International Conference}} on {{System Engineering}} and {{Technology}} ({{ICSET}})},
  isbn = {978-1-72810-758-5},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\DFIYEQTA\\Samsuden et al_2019_A Review Paper on Implementing Reinforcement Learning Technique in Optimising.pdf}
}

@thesis{allisSearchingSolutionsGames1994,
  title = {Searching for solutions in games and artificial intelligence},
  author = {Allis, Louis Victor},
  date = {1994},
  institution = {{Ponsen \& Looijen}},
  location = {{Wageningen}},
  isbn = {9789090074887},
  langid = {Summary in Dutch},
  annotation = {OCLC: 60586781},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\YN2U4Q8X\\Allis_1994_Searching for solutions in games and artificial intelligence.pdf}
}

@article{crowleyFlexibleStrategyUse1993,
  title = {Flexible {{Strategy Use}} in {{Young Children}}'s {{Tic-Tac-Toe}}},
  author = {Crowley, Kevin and Siegler, Robert S.},
  date = {1993-10},
  journaltitle = {Cognitive Science},
  volume = {17},
  number = {4},
  pages = {531--561},
  issn = {03640213},
  doi = {10.1207/s15516709cog1704_3},
  url = {http://doi.wiley.com/10.1207/s15516709cog1704_3},
  urldate = {2022-01-02},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\ZLJEDPZ7\\Crowley_Siegler_1993_Flexible Strategy Use in Young Children's Tic-Tac-Toe.pdf}
}

@unpublished{block-berlitzm.ProInformatikFunktionaleProgrammierung2009,
  type = {Vorlesung},
  title = {ProInformatik - Funktionale Programmierung: Vom Amateur zum Großmeister - von Spielbäumen und anderen Wäldern},
  author = {{Block-Berlitz, M.}},
  year = {2009},
  url = {https://www.inf.fu-berlin.de/lehre/SS09/PI02/docs/MinMax.pdf},
  urldate = {2022-02-22},
  langid = {german},
  venue = {{Freie Universität Berlin SoSe 2009}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\IYSIJXFL\\MinMax.pdf}
}

@unpublished{kontesg.SeminarReinforcementLearning2021,
  type = {Seminar},
  title = {Seminar: {{Reinforcement Learning Foundations}}},
  author = {{Kontes, G.}},
  date = {2021-05-27},
  langid = {english},
  venue = {{Fraunhofer IIS}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\V5LQ4AM5\\RL1_LAB_Foundations_2_Main.pptx}
}

@article{suttonLearningPredictMethods1988,
  title = {Learning to {{Predict}} by the {{Methods}} of {{Temporal Differences}}},
  author = {Sutton, Richard S.},
  date = {1988},
  journaltitle = {Machine Learning},
  volume = {3},
  number = {1},
  pages = {9--44},
  issn = {08856125},
  doi = {10.1023/A:1022633531479},
  keywords = {_tablet},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\TGM8JV6J\\Sutton_1988_Learning to Predict by the Methods of Temporal Differences.pdf}
}

@article{watkinsQlearning1992,
  title = {Q-Learning},
  author = {Watkins, Christopher J. C. H. and Dayan, Peter},
  date = {1992-05-01},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {8},
  number = {3},
  pages = {279--292},
  issn = {1573-0565},
  doi = {10.1007/BF00992698},
  abstract = {Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.},
  langid = {english},
  keywords = {_tablet},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\NL2F9TBG\\Watkins_Dayan_1992_Q-learning.pdf}
}

@thesis{watkinsLearningDelayedRewards1989,
  title = {Learning {{From Delayed Rewards}}},
  author = {Watkins, Christopher},
  date = {1989-01-01},
  institution = {{King's College London}},
  abstract = {Photocopy. Supplied by British Library. Thesis (Ph. D.)--King's College, Cambridge, 1989.},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\2A9BS78I\\Watkins_1989_Learning From Delayed Rewards.pdf}
}

@online{IrelandComparisonThereFundamental,
  title = {Comparison - {{Is}} There a Fundamental Difference between an Environment Being Stochastic and Being Partially Observable?},
  author = {Ireland, David},
  url = {https://ai.stackexchange.com/a/33889/51242},
  urldate = {2022-01-19},
  date = {2021},
  organization = {{Artificial Intelligence Stack Exchange}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\MBT8UZYN\\is-there-a-fundamental-difference-between-an-environment-being-stochastic-and-be.html}
}

@book{millingtonArtificialIntelligenceGames2009,
  title = {Artificial Intelligence for Games},
  author = {Millington, Ian and Funge, John David},
  date = {2009},
  edition = {2nd ed},
  publisher = {{Morgan Kaufmann/Elsevier}},
  location = {{Burlington, MA}},
  isbn = {978-0-12-374731-0},
  pagetotal = {870},
  keywords = {Artificial intelligence,Computer animation,Computer games,Programming},
  annotation = {OCLC: ocn319064669},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\6FZFFTKH\\Millington_Funge_2009_Artificial intelligence for games.pdf}
}

@article{v.neumannZurTheorieGesellschaftsspiele1928,
  title = {Zur Theorie der Gesellschaftsspiele},
  author = {v. Neumann, J.},
  options = {useprefix=true},
  date = {1928-12},
  journaltitle = {Mathematische Annalen},
  shortjournal = {Math. Ann.},
  volume = {100},
  number = {1},
  pages = {295--320},
  issn = {0025-5831, 1432-1807},
  doi = {10.1007/BF01448847},
  langid = {german}
}

@article{shannonXXIIProgrammingComputer1950,
  title = {{{XXII}}. {{Programming}} a {{Computer}} for {{Playing Chess}} 1},
  author = {Shannon, C. and Telephone, Bell},
  date = {1950},
  doi = {10.1080/14786445008521796},
  abstract = {This paper is concerned with the problem of constructing a computing routine or "program" for a modern general purpose computer which will enable it to play chess. (1950). XXII. Programming a computer for playing chess. The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science: Vol. 41, No. 314, pp. 256-275.},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\LK2HI3DK\\Shannon_Telephone_1950_XXII.pdf}
}

@incollection{slateCHESSNorthwesternUniversity1983,
  title = {{{CHESS}} 4.5—{{The Northwestern University}} Chess Program},
  booktitle = {Chess {{Skill}} in {{Man}} and {{Machine}}},
  author = {Slate, David J. and Atkin, Lawrence R.},
  editor = {Frey, Peter W.},
  date = {1983},
  pages = {82--118},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4612-5515-4_4},
  isbn = {978-0-387-90815-1},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\3EDZ4PWH\\Slate_Atkin_1983_CHESS 4.pdf}
}

@incollection{greenblattGreenblattChessProgram1988,
  title = {The {{Greenblatt Chess Program}}},
  booktitle = {Computer {{Chess Compendium}}},
  author = {Greenblatt, Richard D. and Eastlake, Donald E. and Crocker, Stephen D.},
  editor = {Levy, David},
  date = {1988},
  pages = {56--66},
  publisher = {{Springer New York}},
  location = {{New York, NY}},
  doi = {10.1007/978-1-4757-1968-0_7},
  isbn = {978-1-4757-1970-3},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\4VD6DKQZ\\Greenblatt et al_1988_The Greenblatt Chess Program.pdf}
}

@book{gardnerm.HexaflexagonsOtherMathematical1988,
  title = {Hexaflexagons and Other Mathematical Diversions: The First {{Scientific American}} Book of Puzzles \& Games},
  shorttitle = {Hexaflexagons and Other Mathematical Diversions},
  author = {{Gardner, M.}},
  date = {1988},
  publisher = {{University of Chicago Press}},
  location = {{Chicago}},
  isbn = {978-0-226-28254-1},
  pagetotal = {200},
  keywords = {Mathematical recreations},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\626DP2SE\\Gardner_1988_Hexaflexagons and other mathematical diversions.pdf}
}

@book{wangPopularLecturesMathematical2014,
  title = {Popular {{Lectures}} on {{Mathematical Logic}}.},
  author = {Wang, Hao},
  date = {2014},
  publisher = {{Dover Publications}},
  abstract = {A noted logician and philosopher addresses various forms of mathematical logic, discussing both theoretical underpinnings and practical applications. Author Hao Wang surveys the central concepts and theories of the discipline in a historical and developmental context, and then focuses on the four principal domains of contemporary mathematical logic: set theory, model theory, recursion theory and constructivism, and proof theory. Topics include the place of problems in the development of theories of logic and logic's relation to computer science. Specific attention is given to G??del's incompleteness theorems, predicate logic and its decision and reduction problems, constructibility and Cantor's continuum hypothesis, proof theory and Hilbert's program, hierarchies and unification, proof of the four-color problem, the Diophantine problem, the tautology problem, and many other subjects. Three helpful Appendixes conclude the text.},
  isbn = {978-1-322-18384-8},
  langid = {english},
  annotation = {OCLC: 893188891}
}

@book{newellHumanProblemSolving1972,
  title = {Human Problem Solving},
  author = {Newell, Allen and Simon, Herbert A.},
  date = {1972},
  publisher = {{Prentice-Hall}},
  location = {{Englewood Cliffs, N.J}},
  isbn = {978-0-13-445403-0},
  pagetotal = {920},
  keywords = {Human information processing,Problem solving}
}

@online{sebglavCreatingTicTacToeBoards2022,
  type = {Forum},
  title = {Creating {{Tic-Tac-Toe}} Boards with {{LaTeX}}/{{TikZ}} without Tables},
  author = {{SebGlav}},
  date = {2022-02-21},
  url = {https://tex.stackexchange.com/questions/634666/creating-tic-tac-toe-boards-with-latex-tikz-without-tables},
  urldate = {2022-02-22},
  organization = {{TeX - LaTeX Stack Exchange}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\GJXAB5EJ\\creating-tic-tac-toe-boards-with-latex-tikz-without-tables.html}
}

@online{kutscheraa.BestOpeningMove2018,
  title = {The Best Opening Move in a Game of Tic-Tac-Toe},
  author = {{Kutschera, A.}},
  date = {2018},
  url = {http://blog.maxant.co.uk/pebble/2018/04/07/1523086680000.html},
  urldate = {2022-02-16},
  organization = {{The Kitchen in the Zoo}}
}

@online{vj.l.TikzPgfFollowup,
  title = {Tikz Pgf - {{Follow-up}}: {{Drawing}} (a Partial) Game Tree of {{Tic-Tac-Toe}}},
  shorttitle = {Tikz Pgf - {{Follow-up}}},
  author = {V, J. L.},
  url = {https://tex.stackexchange.com/q/634724/220502},
  urldate = {2022-02-22},
  date = {2022},
  organization = {{TeX - LaTeX Stack Exchange}}
}

@online{soemersd.GameAiHow,
  title = {Game Ai - {{How}} Can Both Agents Know the Terminal Reward in Self-Play Reinforcement Learning?},
  author = {{Soemers, D.}},
  url = {https://ai.stackexchange.com/a/6574/51242},
  urldate = {2022-01-19},
  date = {2018},
  organization = {{Artificial Intelligence Stack Exchange}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\8P35W8T5\\how-can-both-agents-know-the-terminal-reward-in-self-play-reinforcement-learning.html}
}

@online{mirnovi.QLearningTicTacToe2020,
  title = {Q-{{Learning}} and {{Tic-Tac-Toe}}},
  author = {{Mirnov, I.}},
  date = {2020},
  url = {http://www.iliasmirnov.com/ttt/},
  urldate = {2022-02-23},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\9UXAQFLJ\\ttt.html}
}

@online{nbroReinforcementLearningHow,
  title = {Reinforcement Learning - {{How}} Are Afterstate Value Functions Mathematically Defined?},
  author = {{nbro}},
  url = {https://ai.stackexchange.com/questions/24816/how-are-afterstate-value-functions-mathematically-defined},
  urldate = {2021-12-18},
  date = {2020},
  organization = {{Artificial Intelligence Stack Exchange}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\NGWXWH5B\\how-are-afterstate-value-functions-mathematically-defined.html}
}

@article{epsteinIdealTrainer1994,
  title = {Toward an Ideal Trainer},
  author = {Epstein, Susan L.},
  date = {1994-06},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {15},
  number = {3},
  pages = {251--277},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00993346},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\NEQ3X72F\\Epstein_1994_Toward an ideal trainer.pdf}
}

@online{digiovanniSurveySelfPlayReinforcement2021,
  title = {Survey of {{Self-Play}} in {{Reinforcement Learning}}},
  author = {DiGiovanni, Anthony and Zell, Ethan C.},
  date = {2021-07-06},
  eprint = {2107.02850},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/2107.02850},
  urldate = {2022-02-01},
  abstract = {In reinforcement learning (RL), the term self-play describes a kind of multi-agent learning (MAL) that deploys an algorithm against copies of itself to test compatibility in various stochastic environments. As is typical in MAL, the literature draws heavily from well-established concepts in classical game theory and so this survey quickly reviews some fundamental concepts. In what follows, we present a brief survey of self-play literature, its major themes, criteria, and techniques, and then conclude with an assessment of current shortfalls of the literature as well as suggestions for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Science and Game Theory},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\TVNZTGT8\\DiGiovanni_Zell_2021_Survey of Self-Play in Reinforcement Learning.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\ADSMV6LS\\2107.html}
}

@report{boyanModularNeuralNetworks1992,
  title = {Modular Neural Networks for Learning Context-Dependent Game Strategies},
  author = {Boyan, Justin A.},
  date = {1992},
  institution = {{Master’s thesis, Computer Speech and Language Processing}},
  abstract = {by},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\G2AFAH2H\\Boyan_1992_Modular neural networks for learning context-dependent game strategies.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\NJJZ6ELM\\summary.html}
}

@report{bowlingConvergenceNoRegretMultiagent2004,
  title = {Convergence and {{No-Regret}} in {{Multiagent Learning}}},
  author = {Bowling, Michael},
  date = {2004},
  institution = {{University of Alberta Libraries}},
  doi = {10.7939/R3ZS2KF41},
  url = {https://era.library.ualberta.ca/items/2225eedc-947a-4ff0-bba4-8c3bd49ddc86},
  urldate = {2021-12-04},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\Q5UHAWCR\\Bowling_2004_Convergence and No-Regret in Multiagent Learning.pdf}
}

@online{slatern.GameAiWhy,
  title = {Game Ai - {{Why}} Is Tic-Tac-Toe Considered a Non-Deterministic Environment?},
  author = {{Slater, N.}},
  url = {https://ai.stackexchange.com/questions/22837/why-is-tic-tac-toe-considered-a-non-deterministic-environment},
  urldate = {2021-12-21},
  date = {2020},
  organization = {{Artificial Intelligence Stack Exchange}},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\38GF2EVD\\why-is-tic-tac-toe-considered-a-non-deterministic-environment.html}
}

@article{parkNovelLearningRate2020,
  title = {A {{Novel Learning Rate Schedule}} in {{Optimization}} for {{Neural Networks}} and {{It}}’s {{Convergence}}},
  author = {Park, Jieun and Yi, Dokkyun and Ji, Sangmin},
  date = {2020-04-22},
  journaltitle = {Symmetry},
  shortjournal = {Symmetry},
  volume = {12},
  number = {4},
  pages = {660},
  issn = {2073-8994},
  doi = {10.3390/sym12040660},
  url = {https://www.mdpi.com/2073-8994/12/4/660},
  urldate = {2022-02-03},
  abstract = {The process of machine learning is to find parameters that minimize the cost function constructed by learning the data. This is called optimization and the parameters at that time are called the optimal parameters in neural networks. In the process of finding the optimization, there were attempts to solve the symmetric optimization or initialize the parameters symmetrically. Furthermore, in order to obtain the optimal parameters, the existing methods have used methods in which the learning rate is decreased over the iteration time or is changed according to a certain ratio. These methods are a monotonically decreasing method at a constant rate according to the iteration time. Our idea is to make the learning rate changeable unlike the monotonically decreasing method. We introduce a method to find the optimal parameters which adaptively changes the learning rate according to the value of the cost function. Therefore, when the cost function is optimized, the learning is complete and the optimal parameters are obtained. This paper proves that the method ensures convergence to the optimal parameters. This means that our method achieves a minimum of the cost function (or effective learning). Numerical experiments demonstrate that learning is good effective when using the proposed learning rate schedule in various situations.},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\EY7G74VS\\Park et al_2020_A Novel Learning Rate Schedule in Optimization for Neural Networks and It’s.pdf}
}

@article{hunterMatplotlib2DGraphics2007,
  title = {Matplotlib: {{A 2D Graphics Environment}}},
  shorttitle = {Matplotlib},
  author = {Hunter, John D.},
  date = {2007},
  journaltitle = {Computing in Science \& Engineering},
  shortjournal = {Comput. Sci. Eng.},
  volume = {9},
  number = {3},
  pages = {90--95},
  issn = {1521-9615},
  doi = {10.1109/MCSE.2007.55},
}

@inproceedings{littmanMarkovGamesFramework1994,
  title = {Markov Games as a Framework for Multi-Agent Reinforcement Learning},
  booktitle = {In {{Proceedings}} of the {{Eleventh International Conference}} on {{Machine Learning}}},
  author = {Littman, Michael L.},
  date = {1994},
  pages = {157--163},
  publisher = {{Morgan Kaufmann}},
  doi = {10.1.1.48.8623},
  abstract = {In the Markov decision process (MDP) formalization of reinforcement learning, a single adaptive agent interacts with an environment defined by a probabilistic transition function. In this solipsistic view, secondary agents can only be part of the environment and are therefore fixed in their behavior. The framework of Markov games allows us to widen this view to include multiple adaptive agents with interacting or competing goals. This paper considers a step in this direction in which exactly two agents with diametrically opposed goals share an environment. It describes a Q-learning-like algorithm for finding optimal policies and demonstrates its application to a simple two-player game in which the optimal policy is probabilistic.},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\MALGHUX9\\Littman_1994_Markov games as a framework for multi-agent reinforcement learning.pdf;C\:\\Users\\Bingel\\Zotero\\storage\\UPQ6WYZV\\summary.html}
}

@inproceedings{konenParameterSelection2008,
  title = {Reinforcement Learning: {{Insights}} from Interesting Failures in Parameter Selection},
  booktitle = {Parallel Problem Solving from Nature – {{PPSN}} x},
  author = {Konen, Wolfgang and Bartz–Beielstein, Thomas},
  date = {2008},
  pages = {478--487},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  isbn = {978-3-540-87700-4},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\LB959ZBM\\Konen_Bartz–Beielstein_2008_Reinforcement learning.pdf}
}

@article{netoSingleAgentMultiAgentReinforcement,
  title = {From {{Single-Agent}} to {{Multi-Agent Reinforcement Learning}}: {{Foundational Concepts}} and {{Methods}}},
  author = {Neto, Goncalo},
  pages = {61},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\VKHR52UV\\Neto - From Single-Agent to Multi-Agent Reinforcement Lea.pdf}
}

@article{singhReinforcementLearningReplacing1996,
  title = {Reinforcement Learning with Replacing Eligibility Traces},
  author = {Singh, Satinder P. and Sutton, Richard S.},
  date = {1996},
  journaltitle = {Machine Learning},
  shortjournal = {Mach Learn},
  volume = {22},
  number = {1-3},
  pages = {123--158},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/BF00114726},
  langid = {english},
  file = {C\:\\Users\\Bingel\\Zotero\\storage\\NZ8QJW38\\Singh_Sutton_1996_Reinforcement learning with replacing eligibility traces.pdf}
}