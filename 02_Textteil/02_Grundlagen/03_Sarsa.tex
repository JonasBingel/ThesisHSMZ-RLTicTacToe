\subsection{\sarsa}
Der \ac{TD} Algorithmus Sarsa verwendet das Konzept von \ac{TD} Prediction, um die optimale Policy $\pi_*$ zu ermitteln. 
Statt der State-Value Funktion $v_\pi$ approximiert Sarsa die Action-Value Funktion $q_\pi$. 
Der Schätzwert der Action-Value Funktion $q_\pi$ ist $Q$. \cite[S. S. 129]{suttonReinforcementLearningIntroduction2018}
Die Action-Value Funktion $q_\pi(s,a)$ weist \ac{SA Tupel} einen \qValue zu. 
Der \qValue ist der kumulierte Reward, den der Agent erwarten kann, wenn er im Zustand $s$ die Aktion $a$ wählt und danach der Policy $\pi$ folgt. 
Für die Action-Value Funktion gilt daher auch die Beziehung aufeinander folgender Returns, sodass das \ac{TD} Update wie folgt formuliert werden kann \cite[S. 129f.]{suttonReinforcementLearningIntroduction2018}:

\begin{equation}
    \label{eq:sarsa_update}
    \equationentry{TD Update Sarsa}
    Q(S_t,A_t) \leftarrow Q(S_t,A_t) +\alpha[R_{t+1} + \gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]
\end{equation}

Um das Update durchführen benötigt Sarsa das Tupel $(S_t,A_t,R_{t+1},S_{t+1},A_{t+1})$, was der Ursprung für den Namen des Algorithmus ist. 
Auf Basis der Update-Regel kann Sarsa eine Schätzung $Q$ der Action-Value Funktion zur Policy $\pi$ aufstellen, nach der Sarsa mit der Umgebung interagiert. \cite[S. 129f.]{suttonReinforcementLearningIntroduction2018}
Der Agent verwaltet die Abbildung von \ac{SA Tupel} auf ihren geschätzten \qValue in einer \qtable. 
Mittels dieser kann Sarsa für jeden Zustand die Aktion ermitteln, die im höchsten \qValue resultiert.
Da Sarsa die \qValues mit jeder neuen Episode aktualisiert, kann Sarsa zur Interaktion mit der Umgebung einer Greedy-Policy folgen.
Bei einer Greedy-Policy wird immer die Aktion gewählt, die den höchsten \qValue hat.\footnote{Gibt es mehrere Aktionen mit dem höchsten\qValue, wird willkürlich eine der Aktionen gewählt}
Dadurch wird iterativ die Action-Value Funktion verbessert und die Policy nähert sich optimalen Policy $\pi_*$. \cite[S. 129f.]{suttonReinforcementLearningIntroduction2018}

Würde Sarsa ausschließlich gemäß einer Greedy-Policy mit der Umgebung interagieren, könnte der Agent in lokalen Maxima verweilen und würde nicht die optimale Policy lernen. 
Eine mögliche Lösung ist die $\epsilon$-greedy-Policy. 
Die $\epsilon$-greedy-Policy wählt mit einer Wahrscheinlichkeit von $\epsilon$ eine zufällige Aktion und $1-\epsilon$ die Aktion mit dem höchsten \qValue.
Der Hyperparameter $\epsilon$ ist die Explorationswahrscheinlichkeit. \cite[S. 28f.]{suttonReinforcementLearningIntroduction2018}

Damit Sarsa zur optimalen Policy $\pi_*$ konvergiert, muss neben den Konvergenzbedingungen von TD Prediction das Epsilon während dem Training gegen 0 konvergieren \cite[S. 129]{suttonReinforcementLearningIntroduction2018}. 
Bei der Wahl der Explorationswahrscheinlichkeit $\epsilon$ muss eine Balance zwischen Exploration und Exploitation, \dahe Ausnutzung der Aktionen mit höchsten \qValue, ermittelt werden. 
Das Abwägen beider Aspekte wird als Explorations-Exploitations-Dilemma bezeichnet \cite[S. 3]{suttonReinforcementLearningIntroduction2018}.

Der Ablauf des Sarsa Algorithmus ist als Pseudocode formuliert in Algorithmus \ref{algo_sarsa}.\footnote{eigene Darstellung in Anlehnung an \cite[S. 130]{suttonReinforcementLearningIntroduction2018}}

\input{04_Artefakte/04_Algorithmen/pseudocode_sarsa}